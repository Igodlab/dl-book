{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3261345",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81592091",
   "metadata": {},
   "source": [
    "## 3.1 Introduction\n",
    "- Probability theory (Chapter 2) is all about modeling a distribution over observed data outcomes $\\mathcal{D}$ (knowing the parameters $\\boldsymbol{\\theta}$) by computing $p(\\mathcal{D}| \\boldsymbol{\\theta})$\n",
    "- Statistics is the inverse problem. We want to compute $p(\\boldsymbol{\\theta}| \\mathcal{D})$ (so we want to infer the parameters $\\boldsymbol{\\theta}$) given observations. There are two approaches:\n",
    "    - **Frequentist**\n",
    "    - **Bayesian** ($\\leftarrow$ this is king)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a14ba",
   "metadata": {},
   "source": [
    "## 3.2 Bayesian statistics\n",
    "- Observed data $\\mathcal{D}$ is known and fixed, parameters are unknown $\\boldsymbol{\\theta}$ (this is the opposite than frequentist approach (Sec3.3))\n",
    "- We represent our beliefs about the parameters after seing data as a **posterior distribution** (eq.3.1): $p(\\boldsymbol{\\theta}| \\mathcal{D}) = \\frac{p(\\boldsymbol{\\theta})p(\\mathcal{D}|\\boldsymbol{\\theta})}{p(\\mathcal{D})} = \\frac{p(\\boldsymbol{\\theta})p(\\mathcal{D}|\\boldsymbol{\\theta})}{\\int p(\\boldsymbol{\\theta}^\\prime)p(\\mathcal{D}|\\boldsymbol{\\theta}^\\prime)d\\boldsymbol{\\theta}^\\prime}$\n",
    "    - **posterior dist**:  $p(\\boldsymbol{\\theta}| \\mathcal{D})$\n",
    "    - **prior dist**: $p(\\boldsymbol{\\theta})$\n",
    "    - **likelihood**: $p(\\mathcal{D}|\\boldsymbol{\\theta})$\n",
    "    - **marginal dist**: $p(\\mathcal{D})$\n",
    "    \n",
    "### 3.2.1 Tossing coins\n",
    "- this is the 'atom' example of probabilities\n",
    "- We record the outcomes of observed data as $\\mathcal{D}=\\{y_n\\in\\{0,1\\}:n=1:N \\}$\n",
    "\n",
    "#### 3.2.1.1 Likelihood\n",
    "- In a simple coin toss, data is iid, and thus the **sufficient statistics** are $(N_1, N_0=N-N_1)$ in:\n",
    "$p(\\boldsymbol{\\theta}| \\mathcal{D}) = \\prod_n^N\\theta^{y_n}(1-\\theta)^{1-y_n}=\\theta^{N_1}(1-\\theta)^{N_0}$\n",
    "    - $y_n$ is the prob of seing heads at toss number $n$\n",
    "    - *note that sufficient statistics $\\neq\\boldsymbol{\\theta}$, sufficient stats refer to the quantities that capture enough info about the data to be able to estimate the parameters\n",
    "- simple coint toss posterior can also be computed using a *Binomial dist*: $p(\\boldsymbol{\\theta}| \\mathcal{D}) = \\operatorname{Bin}(y|N, \\theta)$\n",
    "\n",
    "#### 3.2.1.2 Prior\n",
    "- We can write an **uninformative prior** using a *uniform dist*, but the *beta dist* it is more general: $p(\\theta)=\\operatorname{Beta}(\\theta|\\breve{\\alpha},\\breve{\\beta}) \\propto \\theta^{\\breve\\alpha-1}(1-\\theta)^{\\breve\\beta-1}$ \n",
    "    - $\\breve{\\alpha},\\breve{\\beta}$ are **hyperparameters** (params of the prior that determine our belief about $\\boldsymbol{\\theta}$), if $\\breve{\\alpha}=\\breve{\\beta}=1$ we recover the uniform dist\n",
    "\n",
    "\n",
    "#### 3.2.1.3 Posterior\n",
    "- $\\text{posterior}\\propto\\text{likelihood}\\times\\text{prior}$\n",
    "- continuing the example of a beta prior, we have a **congugate prior** because the posterior has the same functional form:\n",
    "    $p(\\theta \\mid \\mathcal{D}) \\propto \\theta^{N_1}(1-\\theta)^{N_0} \\theta^{\\breve{\\alpha}-1}(1-\\theta)^{\\breve{\\beta}-1} \\propto \\operatorname{Beta}\\left(\\theta \\mid \\breve{\\alpha}+N_1, \\breve{\\beta}+N_0\\right)=\\operatorname{Beta}(\\theta \\mid \\widehat{\\alpha}, \\widehat{\\beta})$\n",
    "\n",
    "#### 3.2.1.4 Posterior mode (MAP estimate)\n",
    "- In Bayesian statistics, MAP estimate is the mode of the posterior dist. It gives the most probable value of the parameter $\\hat{\\theta}_{\\text{map}}=\\arg\\max_\\theta p(\\theta\\mid\\mathcal{D})=\\arg\\max_\\theta\\log p(\\theta)+\\arg\\max_\\theta\\log p(\\mathcal{D}|\\theta)$\n",
    "    - for a beta dist prior $\\hat{\\theta}_{\\text {map }}=\\frac{\\breve{\\alpha}+N_1-1}{\\breve{\\alpha}+N_1-1+\\breve{\\beta}+N_0-1}$\n",
    "    - if the prior is a uniform dist we get the MLE $\\hat{\\theta}_{\\text {mle}}$, because $p(\\theta)\\propto 1\\rightarrow \\log p(\\theta)\\propto 0$\n",
    "    - if sample size is low, we can use a stronger prior (more pronounced beta dist) **add-one smoothing**\n",
    "    \n",
    "#### 3.2.1.5 Posterior mean\n",
    "- MAP is the mode, thus ut us a weak \n",
    "\n",
    "#### 3.2.1.6 Posterior variance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a83b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
