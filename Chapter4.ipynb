{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9d77d1",
   "metadata": {},
   "source": [
    "# 4 Graphical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fec635",
   "metadata": {},
   "source": [
    "## 4.1 Introduction\n",
    "- Two key principles for building learning models are: *modularity* & *abstraction*, and probability theory brings both under an aligned approach\n",
    "- **Probabilistic Graphical Models (PGMs)** are a math formalism to reason about parameters that describe probabilistic behaviours\n",
    "    - Based on graphs where nodes are rvs and vertices (or lack of vertices) represent conditional independence between rvs\n",
    "\n",
    "\n",
    "### 4.2 Directed graphical models (Bayes nets)\n",
    "- Based on *Directed Probabilistic Graphical Models (DPGMs)* which are *directed acyclic graphs (DAGs)* aka. **Bayes Nets/belief networks**\n",
    "    - fun fact, they don't have anything to do w/ Bayes, its just a model for reasoning about prob dists\n",
    "\n",
    "\n",
    "### 4.2.1 Representing the joint distribution\n",
    "- A nice property of DAGs is that nodes are ordered such that childs $x_{i}$ always come after predecesor or parent nodes $\\boldsymbol{x}_{\\text{pred}(i)/\\text{par}(i)}$ such that: $x_{i} \\perp \\boldsymbol{x}_{\\text{pred}(i)/\\text{par}(i)}\\mid \\boldsymbol{x}_{\\text{par}(i)}$\n",
    "- Thus, joint dists for any phenomena using prob chains of $N_G$ nodes: $p(\\boldsymbol{x}_{1:N_G})=\\prod_{i=1}^{N_G}p(x_i\\mid\\boldsymbol{x}_{\\text{par}(i)})$\n",
    "    - where $p(x_i\\mid\\boldsymbol{x}_{\\text{par}(i)})$ is the conditional prob dist (CPD) for node $i$ \n",
    "    - KEY ADVANTAGE for expressing dists in this way is that the number of parameters needed is significanlty less.\n",
    "        - eg. if $N_G$ is the number of nodes and rv have $K$ discrete states then in an *unstructured joint prob* we need $O(K^{N_G})$ params to specify the prob of every configuration\n",
    "        - conversely, in a DAG we only need predecesors and parents (say we have at most $N_{P}$ parents) then we only need $O(N_{G}K^{N_{P}+1})$ params\n",
    "\n",
    "\n",
    "\n",
    "### 4.2.2 Examples\n",
    "- Examples of how DPGMs can be useful\n",
    "\n",
    "\n",
    "#### 4.2.2.1 Markov chains\n",
    "- If we are dealing w/ Markov chains then the joint dist is very similar to the joint dist above (Sec.4.2.1), but now time dictates sequence\n",
    "    - for a one-dim Markov model (unigram): $p(\\boldsymbol{x}_{1:T})=p(x_1)\\prod_{t=2}^{T}p(x_t\\mid \\boldsymbol{x}_{1:t-1})$\n",
    "    - for a two-dim Markov model (bigram): $p(\\boldsymbol{x}_{1:T})=p(x_1, x_2)\\prod_{t=3}^{T}p(x_t\\mid \\boldsymbol{x}_{t-2:t-1})$\n",
    "    - where, in either case, the lookup table aka **Conditional Probability Table (CPT)** $\\theta_{jk}$ is bounded to $[0,1]$ & row-normalized\n",
    "    \n",
    "    \n",
    "#### 4.2.2.2 The \"student\" network\n",
    "- See book\n",
    "- This is another exmple, where we want to know the prob of a student taking a class, and all this is depended on 5 params (D: difficulty, I:intelligence, G: grade, L: reccom letter, S: SAT score).\n",
    "    - Joint prob is: $p(D, I, G, L, S)=p()$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
